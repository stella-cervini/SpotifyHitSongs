---
title: "Analysis of the top 200 Spotify songs (2017-2021)"
author:
- Cervini Stella
- Mattia Simone
- Montalbano Daniel
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(moments))
suppressPackageStartupMessages(library(formatR))
suppressPackageStartupMessages(library(distill))
#suppressPackageStartupMessages(library(pandoc))
```

```{css, echo=FALSE}
h1, h2 {
  text-align: center;
}
body {
text-align: justify
}
```

<!-- # Analysis of the top 200 Spotify songs (2017-2021) -->

## What makes a hit song

This essay was written by Cervini Stella, Mattia Simone and Montalbano Daniel.
The goal of this project is to analyse the characteristics of the top
Spotify songs (between 2017-2021 in global) and eventually discover the
common properties that made these songs so popular in the first place.

### 1. Data

The dataset used for project was retrieved from
[Kaggle](https://www.kaggle.com/datasets/younver/spotify-top-200-dataset).

```{R}
raw_dataset <- read.csv("../Dataset/spotify.csv", header=TRUE, sep=";")
```

It contains the Spotify Global weekly top 200 songs between 2017-2021.

```{R}
dim(raw_dataset)
```

This dataset consists of a total of 74.661 rows and 40 columns, each row
representing a track and each column containing a different variable
that describes the entity. Here we have the totality of the material
needed for the analysis.

```{r}
str(raw_dataset, vec.len = 2, strict.width = 'cut')
```

For example, in the datset we can find the information necessary for Spotify to
uniquely identify the song (*track_id*, *album_id*,*artist_id*, ...), the
metrics generated by
[Spotify](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features)
to evaluate the performances of the song/artist/album (*rank*,
*artist_popularity*, ...), the musical characteristics of the song, such
as danceability or tempo and also some time references, which include the
end date of the week the track was in the charts and the release date.\
While inspecting the dataset, we noticed that a large number of rows was
duplicated, due to the fact that a song stayed in the charts for more
weeks. Furthermore, if a tune had two or more authors, it appeared in
the dataset one time for each of them.\
For instance, we can run the following test:

```{r}
redundancy_test <- raw_dataset[raw_dataset$track_id == "5aAx2yezTd8zXrkmtKl66Z", ]
dim(redundancy_test)
```

These facts result in a superfluous amount of data, making the information set
complicated to work with.

#### Clean data

In order to perform our analysis, we modified the dataset, obtaining a
new table based on our needs. We wanted to clean the data, so that the
new dataset consists only of 200 unique songs per week, eliminating the
redundant rows. In addition to that, we considered only the columns
holding the information referred to the musical specifications of the
songs, adding other variables whenever necessary. The query applied on
the original data is the following:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=60)}
dataset <- raw_dataset  %>%
  select(track_id, artist_num, danceability, energy, loudness, speechiness, acousticness, liveness, valence, tempo, artist_popularity, rank, streams, key, mode) %>%
  group_by(track_id) %>%
  mutate(n_weeks = n()/artist_num, art_popularity = mean(artist_popularity)/100, best_rank = min(rank), max_streams = max(streams)) %>%
  distinct(track_id, .keep_all = TRUE) %>%
  ungroup() %>%
  select(!c(track_id, artist_popularity, rank, streams))

write.csv(dataset,"../Dataset/clean_spotify.csv", row.names = FALSE)

clean_Spotify_200 <- read.csv("../Dataset/clean_spotify.csv", header=TRUE, sep=",")
```

```{r}
dim(clean_Spotify_200)
```

```{r}
clean_Spotify_200 <- clean_Spotify_200 %>%
  mutate(key = factor(key), mode = factor(mode))
str(clean_Spotify_200, vec.len = 5, strict.width = 'cut')
```

The new dataset contains the essential data for the analysis and all the
variables are already in a correct data type with respect to the data
they represent. It consists of 4247 rows and 13 column, therefore it
appears less complicated to work with, but still complete.\
Original columns/variables:

-   **artist_num:** number of artists in the track;
-   **danceability:** how suitable a track is for dancing based on a
    combination of musical elements including tempo, rhythm stability,
    beat strength, and overall regularity. A value of 0.0 is least
    danceable and 1.0 is most danceable;
-   **energy:** measure from 0.0 to 1.0 and represents a perceptual
    measure of intensity and activity;
-   **loudness:** overall loudness of a track in decibels (dB). Values
    typically range between -60 and 0 db;
-   **speechiness:** detects the presence of spoken words in a track.
    The more exclusively speech-like the recording (e.g. talk show,
    audio book, poetry), the closer to 1.0 the attribute value;
-   **acousticness:** confidence measure from 0.0 to 1.0 of whether the
    track is acoustic, where 1.0 represents high confidence the track is
    acoustic;
-   **liveness:** presence of an audience in the recording. Higher
    liveness values represent an increased probability that the track
    was performed live. A value above 0.8 provides strong likelihood
    that the track is live;
-   **valence:** measure from 0.0 to 1.0 describing the musical
    positiveness conveyed by a track;
-   **tempo:** overall estimated tempo of a track in beats per minute
    (BPM);
-   **key:** key the track is in. Integers map to pitches using standard
    [Pitch Class](https://en.wikipedia.org/wiki/Pitch_class) notation.
    E.g. 0 = C, 1 = C#/Db, 2 = D, and so on. If no key was detected, the
    value is -1;
-   **mode:** modality (major or minor) of a track, the type of scale
    from which its melodic content is derived. Major is represented by 1
    and minor is 0.

Computed/added columns:

-   **n_weeks:** total number of weeks that a song stayed in chart. It
    is calculated by dividing the occurrence count of a track by the
    artists that compose it:

```{r, eval=FALSE}
n_weeks = n()/artist_num
```

-   **art_popularity:** mean of the *artist_popularity* that compose a
    same track normalized between 0 and 1. It is necessary in order to
    eliminate the duplicates rows due to the precence of more that one
    composer.

```{r,eval=FALSE}
art_popularity = mean(artist_popularity)/100
```

-   **best_rank:** best position a song reached in the charts. It is
    obtained as

```{r,eval=FALSE}
best_rank = min(rank)
```

-   **max_streams:** total number of streams thata song reaches at its
    peak, computed as

```{r,eval=FALSE}
max_streams = max(streams)
```

It interesting to notice that the dataset do not contain any missing
values.

```{r}
length(unique(complete.cases(clean_Spotify_200))) == 1
```

Now that we have gathered all the information we needed in a clean and
consistent dataset, we can start the analysis.

#### Significant columns to analyze

First of all, we chose the columns that seems actually useful to our
descriptive statistics analysis.
We've used the **summary** function to understand which
columns are the most interesting to analyse deeply.\
We thought that
*max_streams* and *n_weeks* columns are the most important and 
relevant to determine the entire performance of a song in the charts.
Then, we chose the columns that could in some way explain the reasons
behind the success of a tune: *danceability*, *energy*, *tempo*, *key*
and *mode*. We made a distinction between quantitative and qualitative
variables and then we computed their most important indices, expecting
that they could help us understand the information the data enclose. For
example, we analyzed them through mean, median, quantile, variance,
standard deviation, range of variation and interquantile range.

### 2. Descriptive statistics

-   ***Quantitative**:* danceability, energy, tempo, n_weeks and
    max_streams

-   ***Qualitative**:* key, mode

#### Frequency: absolute and relative

In order to compute the absolute and relative frequency of the
quantitative continuous variables, we used the functions
**nclass.Sturges** and **nclass.FD** to find to optimal number of
intervals for every distribution. We did not apply these methods on
*key* and *mode* since they are qualitative variables.

```{r}
nclass.Sturges(clean_Spotify_200$n_weeks)
nclass.FD(clean_Spotify_200$n_weeks)
```

We noticed that the nclass.FD function returned always a larger number
of ranges. To simplify, we chose to use the results of the function
nclass.Sturges. Below are shown all the absolute and relative frequency,
including cumulative frequencies, for all the variables.

-   **n_weeks**

```{r tidy=TRUE, tidy.opts=list(width.cutoff=40)}
n_weeks_class <- cut(clean_Spotify_200$n_weeks, breaks = nclass.Sturges(clean_Spotify_200$n_weeks))
n_weeks_abs_freq <- table(n_weeks_class)
n_weeks_rel_freq <- round(n_weeks_abs_freq/sum(n_weeks_abs_freq),4)
n_weeks_summary <- cbind(n_weeks_abs_freq,cumsum(n_weeks_abs_freq), n_weeks_rel_freq,cumsum(n_weeks_rel_freq) )
colnames(n_weeks_summary) <- c(
  "Absolute",
  "Cum. absolute",
  "Relative",
  "Cum. relative"
)
n_weeks_summary
```

-   **danceability**

```{r echo = FALSE}
danceability_class <- cut(clean_Spotify_200$danceability, breaks = nclass.Sturges(clean_Spotify_200$danceability))
danceability_abs_freq <- table(danceability_class)
danceability_rel_freq <- round(danceability_abs_freq/sum(danceability_abs_freq),4)
danceability_summary <- cbind(danceability_abs_freq,cumsum(danceability_abs_freq), danceability_rel_freq,cumsum(danceability_rel_freq))
colnames(danceability_summary) <- c(
  "Absolute",
  "Cum. absolute",
  "Relative",
  "Cum. relative"
)
danceability_summary
```

-   **energy**

```{r echo = FALSE}
energy_class <- cut(clean_Spotify_200$energy, breaks = nclass.Sturges(clean_Spotify_200$energy))
energy_abs_freq <- table(energy_class)
energy_rel_freq <- round(energy_abs_freq/sum(energy_abs_freq),3)
energy_summary <- cbind(energy_abs_freq,cumsum(energy_abs_freq), energy_rel_freq,cumsum(energy_rel_freq))
colnames(energy_summary) <- c(
  "Absolute",
  "Cum. absolute",
  "Relative",
  "Cum. relative"
)
energy_summary
```

-   **tempo**

```{r echo = FALSE}
tempo_class <- cut(clean_Spotify_200$tempo, breaks = nclass.Sturges(clean_Spotify_200$tempo))
tempo_abs_freq <- table(tempo_class)
tempo_rel_freq <- round(tempo_abs_freq/sum(tempo_abs_freq),3)
tempo_summary <- cbind(tempo_abs_freq,cumsum(tempo_abs_freq), tempo_rel_freq,cumsum(tempo_rel_freq))
colnames(tempo_summary) <- c(
  "Absolute",
  "Cum. absolute",
  "Relative",
  "Cum. relative"
)
tempo_summary
```

-   **max_streams**

```{r echo = FALSE}
max_streams_class <- cut(clean_Spotify_200$max_streams, breaks = nclass.Sturges(clean_Spotify_200$max_streams))
max_streams_abs_freq <- table(max_streams_class)
max_streams_rel_freq <- round(max_streams_abs_freq/sum(max_streams_abs_freq),4)
max_streams_summary <- cbind(max_streams_abs_freq,cumsum(max_streams_abs_freq), max_streams_rel_freq,cumsum(max_streams_rel_freq))
colnames(max_streams_summary) <- c(
  "Absolute",
  "Cum. absolute",
  "Relative",
  "Cum. relative"
)
max_streams_summary
```

-   **key**

```{r echo = FALSE}
key_abs_freq <- table(clean_Spotify_200$key)
key_rel_freq <- key_abs_freq/sum(key_abs_freq)
key_summary <- cbind(key_abs_freq,cumsum(key_abs_freq), key_rel_freq,cumsum(key_rel_freq))
colnames(key_summary) <- c(
  "Absolute",
  "Cum. absolute",
  "Relative",
  "Cum. relative"
)
key_summary

```

-   **mode**

```{r echo = FALSE}
mode_abs_freq <- table(clean_Spotify_200$mode)
mode_rel_freq <- mode_abs_freq/sum(mode_abs_freq)
mode_summary <- cbind(mode_abs_freq,cumsum(mode_abs_freq), mode_rel_freq,cumsum(mode_rel_freq))
colnames(mode_summary) <- c(
  "Absolute",
  "Cum. absolute",
  "Relative",
  "Cum. relative"
)
mode_summary
par(mfrow = c(3, 3))
hist(clean_Spotify_200$n_weeks, main = 'Histogram for n_weeks', xlab = 'n_weeks')
hist(clean_Spotify_200$danceability, main = 'Histogram for danceability', xlab = 'danceability')
hist(clean_Spotify_200$energy, main = 'Histogram for energy', xlab = 'energy')
hist(clean_Spotify_200$tempo, main = 'Histogram for tempo', xlab = 'tempo')
hist(clean_Spotify_200$max_streams, main = 'Histogram for max_streams', xlab = 'max_streams')
barplot(key_summary[,1], main = 'Histogram for key', xlab = 'key')
barplot(mode_summary[,1], main = 'Histogram for mode', xlab = 'mode')
```

By looking at the histogram of the variable *n_weeks*, we can say that
most of the songs stayed in the charts only for a few weeks, while it is
less common that a tune stays in top for longer periods of time.\
We know that *danceability* is a measure of how danceable a track is. The
histogram seems to tell us that the more a song in danceable, more are
the chances that the song reaches the top 200. On the other side, the
highest values are less dense, suggesting that very rhythmical songs are
not too popular.\
The frequency distribution of the variable *energy* seems similar to
the one just described. Therefore, for a song to be in the top chart is important
to not be too energetic or chaotic. Recalling that *tempo* refers to the
BPM of a song, here we can observe that the values are not condensed
around a single value, like *danceability* and *energy*, but they are
more spread\
Regarding *max_strams*, we notice that, as seen for the variable
*n_weeks*, the majority of the values is in the lower intervals of the
distribution. For example, the first interval contains the 57% of the
whole distribution. This could mean that is more complex for a song to
stay in chats for longer and therefore have more streams.\
For what concernes the variable *key*, we notice that there is a peak in
the first intervals. In fact, the 33% of the songs is composed in key DO
or RE, while others pitches are not so popular. Lastly, by looking at
the histogram for *mode*, we can notice that most of the song are
composed in Major modality.

#### Empirical Cumulative Distribution Function:

-   **danceability**

```{r}
ecdf(clean_Spotify_200$danceability)
```

-   **energy**

```{r echo = FALSE}
ecdf(clean_Spotify_200$energy)
```

-   **tempo**

```{r echo = FALSE}
ecdf(clean_Spotify_200$tempo)
```

-   **n_weeks**

```{r echo = FALSE}
ecdf(clean_Spotify_200$n_weeks)
```

-   **max_streams**

```{r echo = FALSE}
ecdf(clean_Spotify_200$max_streams)
```

```{r echo = FALSE}
par(mfrow = c(2, 3))
plot(ecdf(clean_Spotify_200$danceability),main="ECDF Danceability", xlab = 'danceability')
plot(ecdf(clean_Spotify_200$energy),main="ECDF Energy", xlab = 'energy')
plot(ecdf(clean_Spotify_200$tempo),main="ECDF Tempo", xlab = 'tempo')
plot(ecdf(clean_Spotify_200$n_weeks),main="ECDF N. Weeks", xlab = 'n_weeks')
plot(ecdf(clean_Spotify_200$max_streams),main="ECDF Max Streams", xlab = 'max_streams')
```

By computing the Empirical Cumulative Distribution Function plots, we
can confirm what discovered with the histograms.\
Regarding *danceability*, *energy* and *tempo* variables, we can notice
that their distributions are more dense in the center of the
distribution. The frequency of *n_weeks* and *max_streams* variables,
instead, is mostly distributed in the lowest intervals. We can explain
the behavior of these two distributions pointing out that most songs
stay in top charts only for few weeks and it is when they obtain the
highest number of streams. That is also the reason *n_weeks* distributions
is more concentrated in the lower intervals.

#### Mean, median and quartiles

In this section, we computed the measures of central tendency for the
quantitative variables.

```{r echo = FALSE}
summary_n_weeks <- cbind(
  min(clean_Spotify_200$n_weeks),
  mean(clean_Spotify_200$n_weeks),
  max(clean_Spotify_200$n_weeks)
)
summary_max_streams <- cbind(
  min(clean_Spotify_200$max_streams),
  mean(clean_Spotify_200$max_streams),
  max(clean_Spotify_200$max_streams)
)
summary_danceability <- cbind(
  min(clean_Spotify_200$danceability),
  mean(clean_Spotify_200$danceability),
  max(clean_Spotify_200$danceability)
)
summary_energy <- cbind(
  min(clean_Spotify_200$energy),
  mean(clean_Spotify_200$energy),
  max(clean_Spotify_200$energy)
)
summary_tempo <- cbind(
  min(clean_Spotify_200$tempo),
  mean(clean_Spotify_200$tempo),
  max(clean_Spotify_200$tempo)
)

my_summary <- rbind(summary_n_weeks, summary_max_streams, summary_danceability, summary_energy, summary_tempo)
colnames(my_summary) <- c("Min", "Mean", "Max")
row.names(my_summary) <- c("n_weeks", "max_streams", "danceability", "energy", "tempo")
my_summary <- round(my_summary,2)
my_summary
```

By observing the table above, we can acknowledge a few features. For
example, *n_weeks* tell us that a song stays in the charts for an
average of 11 weeks, reaching more that 10 millions streams. The most
popular song in the dataset stayed in the top positions for 218 week,
being listened more than 80 million times. For what concernes
*danceability* and *energy*, knowing that they both range from 0 to 1,
we can observe that they never touch the maximum value, but their mean
is slightly higher than the perfect half of their interval (0.5). The
mean of the variable *tempo* tells us that the most popular songs has an
overall tempo of 122 BPM.

```{r echo = FALSE}
summary_n_weeks <- cbind(
  quantile(clean_Spotify_200$n_weeks, probs = c(0.25)),
  median(clean_Spotify_200$n_weeks),
  quantile(clean_Spotify_200$n_weeks, probs = c(0.75))
)
summary_max_streams <- cbind(
  quantile(clean_Spotify_200$max_streams, probs = c(0.25)),
  median(clean_Spotify_200$max_streams),
  quantile(clean_Spotify_200$max_streams, probs = c(0.75))
)
summary_danceability <- cbind(
  quantile(clean_Spotify_200$danceability, probs = c(0.25)),
  median(clean_Spotify_200$danceability),
  quantile(clean_Spotify_200$danceability, probs = c(0.75))
)
summary_energy <- cbind(
  quantile(clean_Spotify_200$energy, probs = c(0.25)),
  median(clean_Spotify_200$energy),
  quantile(clean_Spotify_200$energy, probs = c(0.75))
)
summary_tempo <- cbind(
  quantile(clean_Spotify_200$tempo, probs = c(0.25)),
  median(clean_Spotify_200$tempo),
  quantile(clean_Spotify_200$tempo, probs = c(0.75))
)

my_summary <- rbind(summary_n_weeks, summary_max_streams, summary_danceability, summary_energy, summary_tempo)
colnames(my_summary) <- c("1st Qu.","Median", "3rd Qu.")
row.names(my_summary) <- c("n_weeks", "max_streams", "danceability", "energy", "tempo")
my_summary <- round(my_summary,2)
my_summary
```

This second table complete the information above. The median tell us
which is the central value of the distributions, while the first and the
third quartile mark, respectively, where the 25% and 75% of the
distribution is. For example, we can observe that the 25% of the
*danceability* values is lower that 0.61 and the 75% is lower than
0.79.\
We can understand better this aspect by examine the boxplots displayed
below.

#### Moda ang Gini Index

For our qualitative variables, we computed the moda and the Gini index,
using our own functions.

Moda Function:

```{r}
moda <- function(x){
  abs_freq <- table(x)
  abs_freq[which.max(abs_freq)]
}
```

Gini index function:

```{r}
gini <- function(x){
  rel_freq <- table(x)/length(x)
  1- sum(rel_freq^2)
}
```

##### Column *key*

Here we compute the value of the two indices for the variable *key*.

```{r}
moda(clean_Spotify_200$key)
gini(clean_Spotify_200$key)
```

As discovered above, level 1 is the most frequent. Also, the Gini index
is 0.9074248. We know that if we consider maximum diversity, the Gini
index is $$k-1\over k$$ where k is the number of different levels of the
variable. In this case we have

```{r}
(12-1)/12
```

so we can assert that the variable *key* has maximum diversity.\
We can confirm this conclusion computing the normalized Gini index, that
is

```{r}
12/(12-1)*gini(clean_Spotify_200$key)
```

Since this value in very close to 1, we can assure what declared above.

##### Column *mode*

Here we compute the value of the two indices for the variable *mode*.

```{r}
moda(clean_Spotify_200$mode)
gini(clean_Spotify_200$mode)
```

The moda supports what we noticed in the histogram regarding this variable. On
the other hand, we have that the Gini index is equal to 0.4896512. Computing
maximum diversity and the normalized Gini index, we know that

```{r}
(2-1)/2
```

```{r}
2/(2-1)*gini(clean_Spotify_200$mode)
```

Also in this case we have maximum diversity.

#### Boxplots

To summarize what we have just computed and comprehend better the
distributions of the data, we can plot side by side all the boxplot of
the variables.

```{r}
par(mfrow = c(1, 5))
boxplot(clean_Spotify_200$danceability, xlab = 'danceability', ylab = 'Values')
boxplot(clean_Spotify_200$energy, xlab = 'energy')
boxplot(clean_Spotify_200$tempo, xlab = 'tempo')
boxplot(clean_Spotify_200$n_weeks, xlab = 'n_weeks')
boxplot(clean_Spotify_200$max_streams, xlab = 'max_streams')
```

In this way we can put emphasis on the different scales of the data and
try to discover the first relations between the variables. As we can
see, *danceability* and *energy* have similar values of mean, median,
quartiles and variance. This could be due to the fact that the two
variables are on the same scale and have similar distributions. We can
notice that all the distributions, aside from the variable *tempo*, have
a lot of outliers. This means that many values are numerically distant
from the rest of the data and do not fell well inside the overall
pattern of the data.\
This phenomenon concernes *n_weeks* and *max_streams* the most. The
presence of outliers could be motivated by the reasons exposed above: a
lot of tracks does not reach a top position in the charts and, the ones that does, 
are treated as odd observations.

#### Variance, standard deviation and other variance measures

Here we compute and then discuss the variability of the data.

##### VAR, SD and IQR

In this section, we compute and comment the indices regarding the
variability of the data, that are *variance*, *standard deviation*, *range
of variation*, *interquatile range* and *MAD*.\
We created our own **interquantile_range** function to calculate the
difference between the first and the third quartile:

```{r}
interquartile_range <- function(x){
  diff(quantile(x, probs = c(0.25, 0.75)))
}
```

The table below contains all the indicators listed previously.

```{r echo = FALSE}
Danceability <- cbind(var(clean_Spotify_200$danceability),
                      sd(clean_Spotify_200$danceability),
                      diff(range(clean_Spotify_200$danceability)),
                      interquartile_range(clean_Spotify_200$danceability),
                      mad(clean_Spotify_200$danceability))
Energy <- cbind(var(clean_Spotify_200$energy),
                sd(clean_Spotify_200$energy),
                diff(range(clean_Spotify_200$energy)),
                interquartile_range(clean_Spotify_200$energy),
                mad(clean_Spotify_200$energy))
Tempo <- cbind(var(clean_Spotify_200$tempo),
               sd(clean_Spotify_200$tempo),
               diff(range(clean_Spotify_200$tempo)),
               interquartile_range(clean_Spotify_200$tempo),
               mad(clean_Spotify_200$tempo))
Weeks <- cbind(var(clean_Spotify_200$n_weeks),
               sd(clean_Spotify_200$n_weeks),
               diff(range(clean_Spotify_200$n_weeks)),
               interquartile_range(clean_Spotify_200$n_weeks),
               mad(clean_Spotify_200$n_weeks))
Streams <- cbind(var(clean_Spotify_200$max_streams),
                 sd(clean_Spotify_200$max_streams),
                 diff(range(clean_Spotify_200$max_streams)),
                 interquartile_range(clean_Spotify_200$max_streams),
                 mad(clean_Spotify_200$max_streams))

var_sd <- rbind(Danceability, Energy, Tempo, Weeks, Streams)
colnames(var_sd) <- c('Variance','Standar deviation', 'Diff', 'Interq. Range', 'MAD')
row.names(var_sd) <- c('Danceability', 'Energy', 'Tempo', 'Weeks', 'Streams')
#var_sd <- round(var_sd,5)
var_sd
```

By examining the table, it is not so clear how variable are the
data. Therefore, in order to understand better the variability of the
data, we computed the coefficient of variation.

##### Coefficient of variation

Here we compute the coefficient of variation, implementing our own function.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=50)}
cv<-function(x){
  return(sd(x)/mean(x))
}
cv_column<-rbind(
  cv(clean_Spotify_200$danceability),
  cv(clean_Spotify_200$energy),
  cv(clean_Spotify_200$tempo),
  cv(clean_Spotify_200$n_weeks),
  cv(clean_Spotify_200$max_streams))
row.names(cv_column) <- c('Danceability','Energy', 'Tempo', 'N. Weeks','Max Streams')
colnames(cv_column) <- c('Coefficient of Variation')
cv_column
```

The coefficient of variation is useful because is dimensionless, that is
independent of the unit in which the measurement was taken, and allow to
compare data sets with different units or widely different means.\
cv is the ratio between the standard deviation and the mean, so the
higher the coefficient of variation, the higher the standard deviation
relative to the mean. In this case, we can point out that *n_week* is
the most volatile variable of all the columns.

##### Skewness

Skewness is a measure of the symmetry of the frequency distribution, compared to a standard normal distribution. A negative skewness indicates that the distribution is skewed towards the left while a positive skewness would indicate the that a distribution is right skewed. These features tells us that the data is asymmetric.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=50)}
sk_column<-rbind(
  skewness(clean_Spotify_200$danceability),
  skewness(clean_Spotify_200$energy),
  skewness(clean_Spotify_200$tempo),
  skewness(clean_Spotify_200$n_weeks),
  skewness(clean_Spotify_200$max_streams))
row.names(sk_column) <- c('Danceability','Energy', 'Tempo', 'N. Weeks','Max Streams')
colnames(sk_column) <- c('Skewness')
sk_column
```

The variables *danceability* and *energy* has a negative value as the result of the
computation of the skewness index. Therefore, it indicates that the tail is left-sided. That confirms also that the mean is lower than the median.\
On the other hand, *tempo*, *n_weeks* and *max_streams* has a positive value of skewness, so it means that the tail is towards the right-side. We can also notice that *n_weeks* has the highest index, emphasizing, as seen before, that the vast majority of the values are in the lower part of the distribution.

##### Kurtosis

Kurtosis is a measure of the "tailedness" of the frequency distribution. A negative value for this index indicates a thin tailed distribution. That means that the values of the data are distributed closer to the median than we would expect for a standard normal distribution, while a positive kurtosis value indicates we are dealing with a heavier tailed distribution, where extreme outcomes are more common.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=50)}
k_column<-rbind(
  kurtosis(clean_Spotify_200$danceability),
  kurtosis(clean_Spotify_200$energy),
  kurtosis(clean_Spotify_200$tempo),
  kurtosis(clean_Spotify_200$n_weeks),
  kurtosis(clean_Spotify_200$max_streams))
row.names(k_column) <- c('Danceability','Energy', 'Tempo', 'N. Weeks','Max Streams')
colnames(k_column) <- c('Kurtosis')
k_column
```

All the variables have a positive value as result of computation of
Kurtosis. An higher value of Kurtosis indicates a more pointed distribution.

#### Relationship between variable

Let's start with a global view of all the relationship between all
variables in the dataset:

```{R tidy=TRUE, tidy.opts=list(width.cutoff=50)}
corrplot(cor(clean_Spotify_200 %>% select(!c(key,mode))), type = "upper", order = "alphabet",  tl.col = "black", tl.srt = 45)
```

Looking at the table above, one can immediately see that the most
important relationships are those between the following variables:

-   energy and loudness

-   best_rank and max_streams

-   acousticness and energy

-   acousticness and loudness

-   best_rank and n_weeks

```{R tidy=TRUE, tidy.opts=list(width.cutoff=50)}
par(mfrow = c(3,2))
plot(clean_Spotify_200$energy, clean_Spotify_200$loudness, xlab="energy", ylab="loudness")
plot(clean_Spotify_200$best_rank, clean_Spotify_200$max_streams, xlab="best_rank", ylab="max_streams")
plot(clean_Spotify_200$acousticness, clean_Spotify_200$energy, xlab="acousticness", ylab="energy")
plot(clean_Spotify_200$acousticness, clean_Spotify_200$loudness, xlab="acousticness", ylab="loudness")
plot(clean_Spotify_200$best_rank, clean_Spotify_200$n_weeks, xlab="best_rank", ylab="n_weeks")
```

##### Covariance

An indicator measuring the strength of the linear relationship between
two variables is the **covariance**:

-   **Positive covariance** indicates that two variables tend to move in
    the same direction;

-   **Negative covariance** reveals that two variables tend to move in
    inverse directions;

-   **Zero covariance** indicates that two variables do not have a
    linear relationship.

```{R}
cov(clean_Spotify_200$energy, clean_Spotify_200$loudness)
```

The covariance is at his maximum when $$cov(x,y)=sd(x)*sd(y)$$ and it's
at his minimun when $$cov(x,y)=-sd(x)*sd(y)$$

```{R}
sd(clean_Spotify_200$energy)*sd(clean_Spotify_200$loudness)
```

So in this case the covariance isn't at his minimun and it isn't at his
maximum.

##### Correlation coefficient

To state whether the covariance is small or large, we must therefore
compare it with the product of the mean squared deviations: as a result,
covariance is usually presented directly in its normalized form, called
**pearson correlation coefficient**.

```{R}
cor(clean_Spotify_200$energy, clean_Spotify_200$loudness)
```

##### Results

```{R tidy=TRUE, tidy.opts=list(width.cutoff=50)}
energy_loudness <- cbind(
  cov(clean_Spotify_200$energy, clean_Spotify_200$loudness),
  cor(clean_Spotify_200$energy, clean_Spotify_200$loudness)
)
best_rank_max_streams <- cbind(
  cov(clean_Spotify_200$best_rank, clean_Spotify_200$max_streams),
  cor(clean_Spotify_200$best_rank, clean_Spotify_200$max_streams)
)
acousticness_energy <- cbind(
  cov(clean_Spotify_200$acousticness, clean_Spotify_200$energy),
  cor(clean_Spotify_200$acousticness, clean_Spotify_200$energy)
)
acousticness_loudness <- cbind(
  cov(clean_Spotify_200$acousticness, clean_Spotify_200$loudness),
  cor(clean_Spotify_200$acousticness, clean_Spotify_200$loudness)
)
best_rank_n_weeks <- cbind(
  cov(clean_Spotify_200$acousticness, clean_Spotify_200$energy),
  cor(clean_Spotify_200$acousticness, clean_Spotify_200$energy)
)

cov_cor <- rbind(energy_loudness, best_rank_max_streams, acousticness_energy, acousticness_loudness, best_rank_n_weeks)
colnames(cov_cor) <- c('Covariance','Correlation')
row.names(cov_cor) <- c('energy-loudness', 'best_rank-max_streams', 'acousticness-energy', 'acousticness-loudness', 'best_rank-n_weeks')
cov_cor <- round(cov_cor,3)
cov_cor
```

Analysing the plots and the correlation ratio, there are no very strong
relationships, but someone can be used with a simple linear regression
to make some predictions.

### 3. Regression

#### Simple linear regression

We can use a simple linear regression to make some prediction about the
best rank given the number of streams, equations:

$$y_i= \beta_0 + \beta_1x_{i}$$ Where:

-   $y_i$ is the response variable best_rank

-   $x_{i}$ is the predictor variable max_streams

-   $\beta_i$ are the regression coefficients

```{R tidy=TRUE, tidy.opts=list(width.cutoff=50)}
first_model = lm(best_rank ~ max_streams, data=clean_Spotify_200)
plot(x=clean_Spotify_200$max_streams, y=clean_Spotify_200$best_rank, main="Linear regression", xlab="max_streams", ylab="best_rank")
abline(first_model, col="red")
```

Looking at the plot, we realise that a linear model is too simple to
represent the data.

```{R}
summary(first_model)
```

This is demonstrated by the low value of R-squared (0.5338): it means
that only 53% of the variation in the output variable is explained by
the input variables.

The distribution of residuals has a median of -11.20 with a minimum and
maximum of -61.82 and 296.15, the 75% of the residuals is between -33.94
and 28.89: not a good result.

```{R}
res <- resid(first_model)
par(mfrow = c(1,2))
hist(res)
qqnorm(res)
qqline(res)
```

A histogram and a Q-Q plot help determine whether or not the generated
residuals follow a normal distribution: in this case, the distribution
is not normal.

#### Polynomial **regression**

We then tried to use a more complex polynomial regression model:

$$y_i= \beta_0 + \beta_1x_i + \beta_2x_i^2 + \beta_3x_i^3 + \beta_4x_i^4 + \beta_5x_i^5$$
Where:

-   $y_i$ is the response variable best_rank

-   $x_{i}$ is the predictor variable max_streams

-   $\beta_i$ are the regression coefficients

```{R}
second_model = lm(best_rank ~ poly(max_streams,5), data=clean_Spotify_200)
summary(second_model)
```

The regression function is:
$$y_i= 87.2204 -2803.4201x_i + 1784.3327x_i^2 -1164.1427x_i^3 + 695.6781x_i^4 -369.9682x_i^5$$

```{R tidy=TRUE, tidy.opts=list(width.cutoff=40)}
plot(x=clean_Spotify_200$max_streams, y=clean_Spotify_200$best_rank, main="Exponential regression", xlab="max_streams", ylab="best_rank")
lines(sort(clean_Spotify_200$max_streams),
      fitted(second_model)[order(clean_Spotify_200$max_streams)],
      col = "red",
      type = "l")
```

In this case we have obtained a R-squared value of 0.8843: it means
88.4% of the variation in the output variable is explained by the input
variables.

The distribution of the residuals has a median of -0.633 with a minimum
and maximum of -61.091 and 87.079, the 75% of the residuals is between
-10.247 and 9.317: a good result.

```{R, echo= FALSE}
res <- resid(second_model)
par(mfrow = c(1,2))
hist(res)
qqnorm(res)
qqline(res)
```

The histogram and the Q-Q plot show that the distribution of residuals
is quite normal around 0, but there are anomalies at the extremes.

#### Multivariate

To predict the bast_rank we can use a multivariate linear regression,
consisting of:

-   Seven variables (already normalized between 0 and 1)

-   Regression coefficients

Defined by the equation:

$$y_i= \beta_0 + \beta_1x_{1i} +  \beta_2x_{2i} +  \beta_3x_{3i} +  \beta_4x_{4i} +  \beta_5x_{5i} +  \beta_6x_{6i} +  \beta_7x_{7i}$$

Where:

-   $y_i$ is the response variable best_rank

-   $x_{1i}$ is the danceability variable

-   $x_{2i}$ is the energy variable

-   $x_{3i}$ is the speechiness variable

-   $x_{4i}$ is the acousticness variable

-   $x_{5i}$ is the liveness variable

-   $x_{6i}$ is the valence variable

-   $x_{7i}$ is the art_popularity variable

-   $\beta_i$ are the regression coefficients

```{R tidy=TRUE, tidy.opts=list(width.cutoff=50)}
third_model = lm(best_rank ~ danceability + energy + speechiness + acousticness + liveness + valence + art_popularity, data=clean_Spotify_200)
summary(third_model)
```

Again, a linear model is too simple for the data we have: this is
demonstrated by the low value of R square (0.05347) and the distribution
of residuals: 75% of that are between -50.319 and 46.920.

```{R, echo= FALSE}
res <- resid(third_model)
par(mfrow = c(1,2))
hist(res)
qqnorm(res)
qqline(res)
```

The histogram and the Q-Q plot show that the distribution of residuals
is not normal.

### 4. Tests

In this section, we show some example of tests that we conducted on the
data. After a few tries, we selected the tests that returned the much
significant results that could help us understand the theory behind the
tests analysis.\
In this first part, we used the *t.test* function on the column
*best_rank* to find the confidence interval for its mean at a level of
0.99.

```{r}
t.test(clean_Spotify_200$best_rank, alternative='two.sided', conf.level = 0.99)
```

Here, we obtained a p-value equal to $2.2e-16$, that indeed is
significantly smaller that 0.05. This mean that we cannot cannot accept
the null hypothesis $H_0$, that is true mean equal to zero. We must
accept the alternative hypothesis $H_1$ that state the exact opposite
(true mean is not equal to zero). Knowing that the the true mean is
87.22039, we found the confidence interval, that is
$84.89195 \le 87.22039 \le 89.54883$. This test tells us that, with 99%
of confidence, the mean is in this interval.\
By using the code below, we tried to make some assumptions on the mean.
Knowing that the variable *best_rank* can space between 1 and 200, we
presumed that the unknown mean is equal to 90, which is slightly lower
that the center of the range for the variable (100th position in the
charts).

```{r}
t.test(clean_Spotify_200$best_rank, mu = 90, conf.level = 0.95)
t.test(clean_Spotify_200$best_rank, mu = 90,alternative='greater', conf.level = 0.95)
t.test(clean_Spotify_200$best_rank, mu = 90, alternative='less', conf.level = 0.95)
```

In general, here we reject the null hypothesis is the p-value in lower
that 0.05. In the first test, we wanted to discover is the real mean of
the variable is 90 and the test returns a p-value equal to 0.002109.
Since p is lower than than our threshold, we reject $H_0$ and accept
$H_1$, which specify that the true mean of the data is not equal to 90.\
The other two tests were applied to locate where the true mean of the
data lied, above or below our hypothetical mean of 90. The first inquiry
if the true value is *greater* than 90, the second if it is *less*. We
obtain a p-value of 0.9989 and 0.001055 respectively. So, in the first
case we are accept the null hypothesis while in the second case we $H_0$
and acknowledge the alternative hypothesis instead. Combined, this two
tests reveal that the mean in lower that 90 and included in the interval
(85.73386, 88.70693).\
This last *t.test*, was utilized to test if the two variables
*danceability* and *energy* had the same mean, since they space in the
same interval [0, 1] and in the previous section we notice that they
have similar distributions.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=50)}
t.test(clean_Spotify_200$danceability, clean_Spotify_200$energy, var.equal = FALSE, conf.level = 0.95)
```

This test confirms that, even if they are analogous, their means are
different and given a p-value equal to $2.2e-16$, we sure reject the
null hypothesis and accept $H_1$, that states that the true difference
in means is not equal to 0.\
Afterward, we run the *shapiro.test* function to test if a variable have
a normal distribution. In this case, we chose to apply the test on the
variable *dancebility* since, by observing its histogram, we noticed
that, from all the histograms, it could look more similar to a normally
distributed dataset.

```{r}
shapiro.test(clean_Spotify_200$danceability)
```

The test confute our observation. The p-value is lower than 0.05 and
therefore we must accept the alternative hypothesis: the values are not
normally distributed.

Pearson's Chi-squared test, performed on the contingency table, tests the independence between two variables. If the p-value is higher than 0.05, the null hypothesis can't be rejected.

```{R tidy=TRUE, tidy.opts=list(width.cutoff=50)}
best_rank_chunks <- cut(clean_Spotify_200$best_rank,breaks=nclass.Sturges(clean_Spotify_200$best_rank))
chisq_table <- table(best_rank_chunks,clean_Spotify_200$key)
chisq.test(chisq_table)
```

We used the *chisq.test* function to analyze the independency of two
variables. We computed the p-value using the of the contingency table
based on the *best_rank* and *key* variables. We obtained a p-value of 0.1559
that is grater than 0.05, so the hypothesis $H_0$ can be accepted and we can
state that the *best_rank* and *key* variables are independent.